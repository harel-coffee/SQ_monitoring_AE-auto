{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service Quality Monitoring in Confined Spaces Through Mining Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1: Aspect Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Approaches: Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#general rules\n",
    "from tabulate import tabulate\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# classification tools\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# text-preprocessing tools\n",
    "# python -m spacy download en\n",
    "import spacy\n",
    "\n",
    "# gloabl parameters\n",
    "TRAIN_SET_PATH = \"scs.txt\"\n",
    "TEST_SET_PATH = \"fss.txt\"\n",
    "encoding=\"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "stops = set(stopwords.words('english'))  # nltk stopwords list\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def stemming(texts):\n",
    "    return [[stemmer.stem(word) for word in line] for line in texts]\n",
    "\n",
    "def remove_stop_words(texts):\n",
    "    refined_texts = [[word for word in line if word not in stops] for line in texts]\n",
    "    return refined_texts\n",
    "\n",
    "def remote_punctuation(texts):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    results = [tokenizer.tokenize(\" \".join(line)) for line in texts]\n",
    "    print(results)\n",
    "\n",
    "def process_texts(x):\n",
    "    x1 = lemmatization(x)\n",
    "    x2 = remove_stop_words(x1)\n",
    "    x3 = stemming(x2)\n",
    "    return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "with open(TRAIN_SET_PATH, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"|\")\n",
    "        if (label != '-1'):\n",
    "            X.append(text.split())\n",
    "            y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "X = [tokenizer.tokenize(\" \".join(line)) for line in X]\n",
    "\n",
    "X, y = np.array(process_texts(X)), np.array(y)\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1291, 1077, 897, 835, 827, 722, 706, 601, 300, 142]\n",
      "removing index no: 1291\n",
      "removing index no: 1077\n",
      "removing index no: 897\n",
      "removing index no: 835\n",
      "removing index no: 827\n",
      "removing index no: 722\n",
      "removing index no: 706\n",
      "removing index no: 601\n",
      "removing index no: 300\n",
      "removing index no: 142\n",
      "total examples 1363\n"
     ]
    }
   ],
   "source": [
    "to_be_removed = [i for i,x in enumerate(X) if(len(x)==0)]\n",
    "to_be_removed.sort(reverse=True)\n",
    "print(to_be_removed)\n",
    "\n",
    "for i in to_be_removed:\n",
    "    print('removing index no:',i)\n",
    "    X = np.delete(X, i)\n",
    "    y= np.delete(y, i)\n",
    "\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and a tf-idf version of the same\n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        if len(word2vec)>0:\n",
    "            self.dim=len(word2vec[next(iter(word2vec))])\n",
    "        else:\n",
    "            self.dim=0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf, \n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Online Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from scipy import sparse\n",
    "\n",
    "# def similarity(d, center):\n",
    "#     d_sparse = sparse.csr_matrix(d)\n",
    "#     center_sparse = sparse.csr_matrix(center)\n",
    "#     return cosine_similarity(d_sparse, center_sparse, dense_output=True)\n",
    "\n",
    "# def max_similarity(clusters, centers, d):\n",
    "#     d_sparse = sparse.csr_matrix(d)\n",
    "#     sims=[]\n",
    "#     for k,cluster in clusters.items():\n",
    "#         center_sparse = sparse.csr_matrix(centers[k])\n",
    "#         sims.append(cosine_similarity(d_sparse, center_sparse, dense_output=True))\n",
    "#     maxValue = np.max(sims)\n",
    "#     maxIndex = np.where(sims == maxValue)[0][0]\n",
    "#     return maxIndex, maxValue\n",
    "\n",
    "# def recalc_centroid(X, list_of_ids):\n",
    "#     return np.average(X[list_of_ids],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters={}\n",
    "# centers = {}\n",
    "# numTopic = 0 \n",
    "# sigma = 0.5\n",
    "# maxValue_lst=[]\n",
    "# for i,d in enumerate(X_vectors):\n",
    "#     if len(clusters)==0:\n",
    "#         print('creating a new cluster')\n",
    "#         clusters[numTopic] = []\n",
    "#         centers[numTopic] = []\n",
    "#         clusters[numTopic].append(i)\n",
    "#         centers[numTopic].append(d)\n",
    "#         numTopic += 1\n",
    "#     else:\n",
    "#         maxIndex, maxValue = max_similarity(clusters, centers, d)\n",
    "#         maxValue_lst.append(maxValue)\n",
    "#         if maxValue >= sigma:\n",
    "#             clusters[maxIndex].append(i)\n",
    "#             centers[maxIndex] = recalc_centroid(X_vectors, clusters[maxIndex])\n",
    "#         else:\n",
    "#             if (np.sum(d)>0):\n",
    "#                 print('creating new cluster')\n",
    "#                 clusters[numTopic] = []\n",
    "#                 centers[numTopic] = []\n",
    "#                 clusters[numTopic].append(i)\n",
    "#                 centers[numTopic].append(d)\n",
    "#                 numTopic += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of clusters:',len(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([x for x in maxValue_lst if (x>0.9)])/len(maxValue_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification (SVM & Linear Regression & MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 1190\n",
      "[769]\n",
      "removing index no: 769\n",
      "total examples 1189\n"
     ]
    }
   ],
   "source": [
    "X_TEST, y_TEST = [], []\n",
    "\n",
    "with open(TEST_SET_PATH, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"|\")\n",
    "        if (label != '-1'):\n",
    "            X_TEST.append(text.split())\n",
    "            y_TEST.append(label)\n",
    "            \n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "X_TEST = [tokenizer.tokenize(\" \".join(line)) for line in X_TEST]\n",
    "\n",
    "X_TEST, y_TEST = np.array(process_texts(X_TEST)), np.array(y_TEST)\n",
    "print (\"total examples %s\" % len(y_TEST))\n",
    "\n",
    "to_be_removed = [i for i,x in enumerate(X_TEST) if(len(x)==0)]\n",
    "to_be_removed.sort(reverse=True)\n",
    "print(to_be_removed)\n",
    "\n",
    "for i in to_be_removed:\n",
    "    print('removing index no:',i)\n",
    "    X_TEST = np.delete(X_TEST, i)\n",
    "    y_TEST= np.delete(y_TEST, i)\n",
    "\n",
    "print (\"total examples %s\" % len(y_TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list_train = []\n",
    "for i,v in enumerate(y):\n",
    "    classes = v.split(',')\n",
    "    classes2=[int(cl.strip()) for cl in classes]\n",
    "    y_list_train.append(tuple(classes2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list_test = []\n",
    "for i,v in enumerate(y_TEST):\n",
    "    classes = v.split(',')\n",
    "    classes2=[int(cl.strip()) for cl in classes]\n",
    "    y_list_test.append(tuple(classes2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report_as_df(report):\n",
    "    cl_results = report.split()[4:39]\n",
    "    cl_results_rest = report.split()[39:]\n",
    "    df = pd.DataFrame(columns=['P','R','F'])\n",
    "    for i in range(0,len(cl_results),5):\n",
    "        df.loc[len(df)] = cl_results[i+1:i+4]\n",
    "\n",
    "    for i in range(0,len(cl_results_rest),6):\n",
    "        df.loc[len(df)] = cl_results_rest[i+2:i+5]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_from_results(reports_dic):\n",
    "    mean_result = pd.DataFrame(columns=['P','R','F'])\n",
    "    std_result = pd.DataFrame(columns=['P','R','F'])\n",
    "    for j in range(0,len(reports_dic[1])): # for each aspect\n",
    "        df = pd.DataFrame(columns=['P','R','F'])\n",
    "        for i in reports_dic: # for each fold\n",
    "            df.loc[len(df)] = list(reports_dic[i].loc[j])\n",
    "        df = df.apply(pd.to_numeric)\n",
    "        mean_result.loc[len(mean_result)] = df.mean()\n",
    "        std_result.loc[len(std_result)] = df.std()\n",
    "    return mean_result, std_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_reports_dic=dict()\n",
    "svm_f1_dic=dict()\n",
    "svm_ROC_dic=dict()\n",
    "\n",
    "lr_reports_dic=dict()\n",
    "lr_f1_dic=dict()\n",
    "lr_ROC_dic=dict()\n",
    "\n",
    "mlp_reports_dic=dict()\n",
    "mlp_f1_dic=dict()\n",
    "mlp_ROC_dic=dict()\n",
    "\n",
    "for i in range(1,11):\n",
    "    \n",
    "    model = Word2Vec(X, size=100, window=3, min_count=5, workers=2)\n",
    "    w2v = {w: vec for w, vec in zip(model.wv.index2word, model.wv.syn0)}\n",
    "    vec = TfidfEmbeddingVectorizer(w2v)\n",
    "    vec.fit(X,y)\n",
    "    X_vectors = vec.transform(X)\n",
    "    X_vectors_TEST = vec.transform(X_TEST)\n",
    "    \n",
    "    #SVM\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_train_transformed = mlb.fit_transform(y_list_train)\n",
    "    clf = OneVsRestClassifier(svm.SVC(kernel='rbf',C=1,gamma=1/len(y_list_train)))\n",
    "    clf = clf.fit(X_vectors, y_train_transformed) \n",
    "    predicted_labels = clf.predict(X_vectors_TEST)\n",
    "    y_test_transformed = mlb.fit_transform(y_list_test)\n",
    "    report = classification_report(y_test_transformed, predicted_labels)\n",
    "    svm_reports_dic[i] = get_classification_report_as_df(report)\n",
    "    svm_f1_dic[i] = f1_score(y_test_transformed, predicted_labels, average='micro')\n",
    "    svm_ROC_dic[i] = roc_auc_score(y_test_transformed, predicted_labels)\n",
    "    \n",
    "    #LR\n",
    "    clf = OneVsRestClassifier(LogisticRegression(random_state=42))\n",
    "    clf = clf.fit(X_vectors, y_train_transformed) \n",
    "    predicted_labels = clf.predict(X_vectors_TEST)\n",
    "    y_test_transformed = mlb.fit_transform(y_list_test)\n",
    "    report = classification_report(y_test_transformed, predicted_labels)\n",
    "    lr_reports_dic[i] = get_classification_report_as_df(report)\n",
    "    lr_f1_dic[i] = f1_score(y_test_transformed, predicted_labels, average='micro')\n",
    "    lr_ROC_dic[i] = roc_auc_score(y_test_transformed, predicted_labels)\n",
    "    \n",
    "    #MLP\n",
    "    clf = OneVsRestClassifier(MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(512), random_state=1))\n",
    "\n",
    "    clf.fit(X_vectors, y_train_transformed)\n",
    "    predicted_labels = clf.predict(X_vectors_TEST)\n",
    "    y_test_transformed = mlb.fit_transform(y_list_test)\n",
    "    report = classification_report(y_test_transformed, predicted_labels)\n",
    "    mlp_reports_dic[i] = get_classification_report_as_df(report)\n",
    "    mlp_f1_dic[i] = f1_score(y_test_transformed, predicted_labels, average='micro')\n",
    "    mlp_ROC_dic[i] = roc_auc_score(y_test_transformed, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_mean, svm_std = mean_std_from_results(svm_reports_dic)\n",
    "lr_mean, lr_std = mean_std_from_results(lr_reports_dic)\n",
    "mlp_mean, mlp_std = mean_std_from_results(mlp_reports_dic)\n",
    "\n",
    "svm_mean.to_excel('w2v_svm_mean.xlsx')\n",
    "svm_std.to_excel('w2v_svm_std.xlsx')\n",
    "\n",
    "lr_mean.to_excel('w2v_lr_mean.xlsx')\n",
    "lr_std.to_excel('w2v_lr_std.xlsx')\n",
    "\n",
    "mlp_mean.to_excel('w2v_mlp_mean.xlsx')\n",
    "mlp_std.to_excel('w2v_mlp_std.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm 0.5\n",
      "lr 0.5203075036391409\n",
      "mlp 0.5690304473664938\n"
     ]
    }
   ],
   "source": [
    "print('svm',np.mean(list(svm_ROC_dic.values())))\n",
    "print('lr',np.mean(list(lr_ROC_dic.values())))\n",
    "print('mlp',np.mean(list(mlp_ROC_dic.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.828</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        P      R      F\n",
       "0   0.402  0.063  0.106\n",
       "1   0.127  0.040  0.058\n",
       "2   0.700  0.451  0.547\n",
       "3   0.866  0.855  0.859\n",
       "4   0.430  0.049  0.087\n",
       "5   0.627  0.069  0.123\n",
       "6   0.019  0.024  0.022\n",
       "7   0.828  0.623  0.710\n",
       "8   0.451  0.220  0.258\n",
       "9   0.748  0.623  0.647\n",
       "10  0.705  0.663  0.668"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
